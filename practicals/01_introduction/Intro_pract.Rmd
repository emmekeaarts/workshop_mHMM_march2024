---
title: "Introduction: fitting a multilevel hidden Markov model"
mainfont: Arial
params:
  answers: true
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: paper
    pandoc_args: --output=Intro_pract.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mHMMbayes)
library(viridis)
library(reshape2)
emotion_data <- readRDS("./data/data_Rowland2020.rds")
```

This is the first practical, where we introduce the dataset and we use it to fit a (2 state) multilevel hidden Markov model. 

You can use your preferred way of working in R to do the practicals. Our preferred way is this:

- Create a new folder with a good name, e.g., `practicals_mHMM` 
- Open RStudio
- Create a new project from RStudio, which you associate with the folder
- Create a `raw_data` subfolder
- Create an R script for the current practical, e.g., `introduction.R`
- Create your well-documented and [well-styled](https://style.tidyverse.org/) code in this R script

The answers to the practicals are available in the `*_answers.html` files. Try to work out the answer yourself (e.g., inspecting help files and examples in them provided in the `mHMMbayes` package, looking at the tutorial vignette available online) before looking at this!

In all practicals in this workshop, we make use of the `mHMMbayes` package, version 1.0.0. You can load the package like 

```{r, eval=FALSE}
library(mHMMbayes)
```

In this practical, we will also use the following packages:

```{r, eval=FALSE}
library(ggplot2)
library(reshape2)
library(viridis)    # optional
```

## The data
We will use an open access dataset from Rowland and Wenzel (2020), available on OSF [`here`](https://osf.io/jmz2n/). The data consists of one-hundred twenty-five undergraduate students from the University of Mainz in Germany that completed a 40-day ambulatory assessment six times a day, reporting on their affective experiences *happy*, *excited*, *relaxed*, *satisfied*, *angry*, *anxious*, *depressed*, and *sad*. These items were scored with a visual analog slider from 0 to 100. Before the collection of the ESM measurements, participants of the study were randomly assigned to a group receiving a weekly mindfulness treatment during the ESM study and a control group. A cleaned version of the data kindly provided in the data archive as part of Haslbeck, Ryan, & Dablander (2023), the repository can be found [`here`](https://github.com/jmbh/EmotionTimeSeries), and the specific dataset we will be working with [`here`](https://github.com/jmbh/EmotionTimeSeries/tree/master/DataClean/Rowland2020). It is an `rds` file, which is a convenient, portable, and fast binary file format for R.

---

**Exercise 1**
Download the dataset and save it in a nice location, e.g., a `raw_data` folder inside your R project.

---

---

**Exercise 2**
Load the dataset in R using the function `readRDS()`. Give the dataset the name `emotion_data`. Then, inspect the first few rows of the data.

```{r, include = params$answers, eval=FALSE}
emotion_data <- readRDS("../../data/data_Rowland2020.rds")
```

```{r, include = params$answers}
# inspect the first few rows
head(emotion_data)
```
---


---

**Exercise 3**
Inspect the data over time using visualization. Most useful is visualizing time sequences for subjects separately, e.g., by using the option `facet_wrap(vars(subj_id))` in ggplot. 

```{r, include = params$answers}
# inspect the first few rows
library(ggplot2)

# restructure the data for ggplot
emotion_data$beep2 <- (emotion_data$dayno - 1) * 6 + emotion_data$beep

ggplot_emotion <- rbind(data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$happy,
                                 emotion = "happy"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$excited,
                                 emotion = "excited"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$relaxed,
                                 emotion = "relaxed"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$satisfied,
                                 emotion = "satisfied"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$angry,
                                 emotion = "angry"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$anxious,
                                 emotion = "anxious"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$depressed,
                                 emotion = "depressed"),
                      data.frame(ID = emotion_data$subj_id, 
                                 beep2 = emotion_data$beep2, 
                                 outcome = emotion_data$sad,
                                 emotion = "sad"))

ggplot_emotion$emotion <- as.factor(ggplot_emotion$emotion)

ggplot(ggplot_emotion[ggplot_emotion$ID %in% c(1:5),]) +
  geom_line( mapping = aes(x = beep2, y = outcome, group = emotion, color = emotion)) +
  facet_wrap(vars(ID), ncol = 1)


```

---

## Setting up the multilevel hidden Markov model 

In this section, you will fit a 2-state multilevel hidden Markov model. For this, you need a `data.frame` in which only the affective variables you want to use in the model are included, plus the subject ID as the first column.  Please feel free to only use a subset of the provided affective variables, but do make sure to select at least 2 affective variables. 

---

**Exercise 4**
Create a dataset `emotion_mHMM` which has the subject ID variable in the first column, followed by only the affective variables you want to use in the hidden Markov model. 

```{r, include = params$answers}
# inspect the first few rows
emotion_mHMM <- data.frame(subj_ID = emotion_data$subj_id,
                           happy = emotion_data$happy,
                           excited = emotion_data$excited,
                           relaxed = emotion_data$relaxed,
                           satisfied = emotion_data$satisfied,
                           angry = emotion_data$angry,
                           anxious = emotion_data$anxious,
                           depressed = emotion_data$depressed,
                           sad = emotion_data$sad)
```

---

---

**Exercise 5**
Set up the first set of model input arguments: `m` for the number of states the model should infer, `n_dep` for the number of dependent variables used for fitting the model, and starting values for gamma and the emission distribution, saved in the objects `start_gamma` and `start_emiss`, respectively.

Note 1: `start_gamma` should be a `m` by `m` matrix and `start_emiss` should be a list with the number of elements equal to the the number of dependent variables used specified in `n_dep`. Each element within the list should be a matrix with 2 columns and `m` rows. The Normal emission distribution means are contained in the first column of this matrix, and the standard deviations of the state specific Normal emission distribution are contained int the second column of the matrix. 

Note 2: Make sure that the starting values are sensible. That is, for gamma, usually the elements contained on the diagonal (i.e., the self-transitions) are larger than the off-diagonal values. With respect to the emission distribution, if we assume a 2-state model, it is sensible to assume that we will have a 'positive emotion' state and a 'negative emotion' state. Define the means of the variable and state specific emission distributions to align with this idea. Do not use equivalent means for one dependent variable over the states! Regarding the standard deviations of the emission distribution, define them such that the emission distribution for state 1 and 2 together cover the entire range of observations (remember, participants could use a slider ranging from 0 to 100) with reasonable support. For example, given our data, it would not make sense to use a standard deviation of 3 (as 95% of the mass of the distribution will only span mean+/- 2 * 3 values out of 100), but a standard deviation of 10 or 15 would make sense. 

```{r, include = params$answers}
# setting up input parameters, part 1
m <- 2
n_dep <- 8

start_gamma <- matrix(c(0.7, 0.3, 
                        0.3, 0.7), byrow = TRUE, ncol = m, nrow = m)

start_emiss <- list(matrix(c(70, 15,                                     # happy
                             30, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(70, 15,                                     # excited
                             30, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(70, 15,                                     # relaxed
                             30, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(70, 15,                                     # satisfied
                             30, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(15, 10,                                     # angry
                             40, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(15, 10,                                     # anxious
                             40, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(15, 10,                                     # depressed
                             40, 15), byrow = TRUE, ncol = 2, nrow = m), 
                    matrix(c(15, 10,                                     # sad
                             40, 15), byrow = TRUE, ncol = 2, nrow = m))
```

---

---

**Exercise 6**
Set up a weakly informative prior for the continuous emission distribution using the function `prior_emiss_cont()` and save the object in `emotion_prior_emiss`. Again, align the values of the hyper-prior means of the Normal distribution with your hypothesized structure of a 'positive emotion' state and a 'negative emotion' state. Do not use equavalent means over states within the same dependent variable. Tip: for an overview of the usage and an example, check the help file (`?prior_emiss_cont`). 

```{r, include = params$answers}
# specifying weakly informative prior for continuous emission distributions

emotion_prior_emiss <- prior_emiss_cont(
  gen = list(m = m, n_dep = n_dep),
  emiss_mu0 = list(matrix(c(70, 30), nrow = 1),  # happy
                   matrix(c(70, 30), nrow = 1),  # excited
                   matrix(c(70, 30), nrow = 1),  # relaxed
                   matrix(c(70, 30), nrow = 1),  # satisfied
                   matrix(c(15, 40), nrow = 1),  # angry
                   matrix(c(15, 40), nrow = 1),  # anxious
                   matrix(c(15, 40), nrow = 1),  # depressed
                   matrix(c(15, 40), nrow = 1)), # sad 
  emiss_K0 = rep(list(1), n_dep),
  emiss_V = rep(list(rep(5^2, m)), n_dep),
  emiss_nu = rep(list(1), n_dep),
  emiss_a0 = rep(list(rep(1.5, m)), n_dep),
  emiss_b0 = rep(list(rep(20, m)), n_dep),
)

```

---

## Fitting the multilevel hidden Markov model 

Now we will use all ingredients to fit a 2 state multilevel hidden Markov model using the function `mHMM()`. For now, we recommend setting the number of iterations `J` as part of the input parameter `mcmc` to 500, and specify a burn in (`burn_in`) of 200. On a reasonably fast laptop, this should take about 3 minutes, assuming you are using all 8 dependent variables as input. In the mean time, you can have a nice conversation with your neighbor or workshop instructor :) . When fitting a multilevel hidden Markov model for research purposes, we recommend suing at least 2.000 iterations, and using traceplots and the Gelman Rubin statistic to ensure convergence with the used number of iterations and the appropriate burn in period, we will discuss this in the second practical of this workshop. 

---

**Exercise 7**
Fit a 2 state multilevel hidden Markov model using the function `mHMM()`, and save your fitted model in an object named `out_2st_emotion`. See below for the general structure of the `mHMM()` function, you only have to fill in the dots. The provided code assumes continuous observations, no level 2 covariates and only a prior on the Normal emission distribution. 

Note: the starting values which need to specified at the argument `start_val` should be a list, which in the first element contains the starting values for the transition probabilities gamma, and in the subsequent elements contain the starting values for the emission distributions for each of the dependent variables. Hence, the list provided to `start_val` should be a list containing 1 + `n_dep` elements. The list cannot be a nested list (i.e., a list containing multiple levels of nesting). 

```{r, eval = FALSE}
# Run a model without covariate(s) and default priors:
out_2st_emotion <- mHMM(s_data = ...,
                data_distr = "continuous",
                gen = list(m = ..., n_dep = ...),
                start_val = ...,
                mcmc = list(J = ..., burn_in = ...))
```


```{r, eval = FALSE, include = params$answers}
# Run a model without covariate(s) and default priors:
out_2st_emotion <- mHMM(s_data = emotion_mHMM,
                        data_distr = "continuous",
                        gen = list(m = m, n_dep = n_dep),
                        start_val = c(list(start_gamma), start_emiss),
                        emiss_hyp_prior = emotion_prior_emiss,
                        mcmc = list(J = 500, burn_in = 200))

```

```{r, echo=FALSE}
out_2st_emotion <- readRDS("./data/out_2st_emotion.rds")
```

---

## Inspecting general model output 

---

**Exercise 8**
Inspect the global output by using the inbuilt `print()` and `summary()` options for a class `mHMM` object created by using the function `mHMM()` within the `mHMMbayes` package. How many subjects were included in the analysis? On average, do students remain within one and the same state for a long time? What mood do each of the states represent? 

```{r,  include = params$answers}
# Inspect general model output
out_2st_emotion 

summary(out_2st_emotion)

```

---

To facilitate easy post-processing (e.g., visualizing), the transition probability gamma and emission distribution parameters can be obtained separately and saved within an object using the functions `obtain_gamma()` and `obtain_emiss()`

---

**Exercise 9**
Save the group level transition probabilities gamma in an object named `gamma_group`, and the group level emission distribution parameters in an object named `emiss_group`, using the functions `obtain_gamma()` and `obtain_emiss()`, respectively. Inspect the objects. 

```{r,  include = params$answers}
# save group level parameters 
gamma_group <- obtain_gamma(out_2st_emotion) 
gamma_group

emiss_group <- obtain_emiss(out_2st_emotion) 
emiss_group

```

---

## Visualizing the obtained output  

Especially when you have a large number of states and/or a large number of dependent variables, visualizing the group level parameters can be very helpful. 

---

**Exercise 10**
Visualize the group level transition probabilities gamma. Tip: for objects created using the function `obtain_gamma()`, an automated plotting function is available. Use it like so: `plot(object)`. In addition, in the answers a suggestion for plotting is given using `ggplot`. 

```{r,  include = params$answers}
# plotting group level gamma 
plot(gamma_group)

# plotting group level gamma using ggplot
library(viridis)

# restructuring data for ggplot
colnames(gamma_group) <- paste0(1:m)
rownames(gamma_group) <- paste0(1:m)

melted_gamma_group <- melt(gamma_group)
colnames(melted_gamma_group) <- c("From_state", "To_state", "prob")
melted_gamma_group$From_state <- factor(melted_gamma_group$From_state)
melted_gamma_group$To_state <- factor(melted_gamma_group$To_state)

# ussing ggplot 
ggplot(data = melted_gamma_group, aes(x=To_state, y = From_state , fill=prob)) + 
  geom_tile(color = "white") +
  scale_fill_distiller(palette = "Spectral", limits = c(0,1), name = "State switching\nprobability") +
  geom_text(aes(To_state, From_state, label = round(prob,2)), color = "white", size = 4) +
  theme_minimal()+ 
  ylab("From mood state") + xlab("To mood state") +
  scale_y_discrete(limits=rev) +
  coord_fixed()


```

---

---

**Exercise 11**
Visualize the group level emission distributions. Various visualizations are possible, for example bar plots depicting the emission distribution means over states and dependent variables, or depicting the densities of the normal emission distributions, combining information on both the mean and standard deviation. 

```{r,  include = params$answers}
# plotting group level emissions, bar plot

## reshaping data for ggplot to make one plot simultaneously for all dependent variables 
## meaning: one matrix containing all state and variable dependent means, with state and mood as variable 
emiss_group <- lapply(emiss_group, function(x,m){rownames(x) <- paste0(1:m);x}, m = m)
emiss_group_melt <- lapply(emiss_group, melt)
emiss_group_melt <- do.call(rbind, emiss_group_melt)
emiss_group_melt <- emiss_group_melt[emiss_group_melt$Var2 == "Mean", c(1, 3)]
colnames(emiss_group_melt) <- c("State", "Mean")
emiss_group_melt$Dep <- factor(c(rep(names(emiss_group), each = m)), levels = names(emiss_group))
emiss_group_melt$State <- factor(emiss_group_melt$State, label = 1:m)

## plot
ggplot(emiss_group_melt, aes(x = State, y = Mean, fill = Dep)) +
         geom_bar(stat="identity") +
  facet_grid(cols = vars(Dep)) + 
  theme_minimal() +
  theme(legend.position="none") +
  xlab("Mood state")

# plotting group level emissions, distribution density
## obtaining probability for observations within the range 1 to 100 given state and variable dependent mean and SD 
### creating output matrix
len <- 200
grid <- seq(1,100, length = len)
emiss_dens <- data.frame(State = factor(rep(rep(1:m, each = len), n_dep), levels = 1:m), 
                         Dep = factor(c(rep(names(emiss_group), each = m * len)), levels = c(names(emiss_group))), 
                         X = rep(grid, m*n_dep), 
                         Prob = NA)

### filling in the probabilities
for(q in 1:n_dep){
  for(i in 1:m){
    emiss_dens$Prob[((q-1) * len * m + (i-1) * len + 1):((q-1) * len * m + (i-1) * len + len)] <- 
      dnorm(emiss_dens$X[((q-1) * len * m + (i-1) * len + 1):((q-1) * len * m + (i-1) * len + len)],
            mean = emiss_group[[q]][i,1], sd = emiss_group[[q]][i,2])
  }
}

## plot 
ggplot(emiss_dens, aes(X, color = State))  + 
  # geom_density() + 
  xlim(c(0,100)) + ylim(c(0,0.05)) +
  geom_line(aes(y = Prob)) +
  geom_vline(data = emiss_group_melt, aes(xintercept = Mean, colour = as.factor(State)), linetype="dashed", linewidth = 0.6) +
  xlab("Mood value") +
  facet_wrap(vars(Dep)) + 
  theme_minimal() 
```

---


## Obtaining the most likely sequence of hidden states 

Using the obtained (subject specific) parameter estimates and the complete sequence of observations for each subject, we can infer the most likely sequence of states for each subject. To obtain the probability of each state at each point in time (where the sequence of states with the highest probability forms the sequence of most likely states), we make use of the Viterbi algorithm (Viterbi, 1967). In the package `mHMMbayes`, (an extended version of) the Viterbi algorithm is implemented in the function `vit_mHMM()`. The required input is the object that contains the fitted model generated by the function `mHMM()`, and the data used as input when fitting the multilevel hidden Markov model. 

---

**Exercise 12**
Using the function `vit_mHMM()`, the object `out_2st_emotion` containing our fitted multilevel HMM model, and the data `emotion_mHMM`, obtain the most likely hidden state sequence. Save the state sequences in the object `emotion_states_2st`, and inspect the object. 

```{r,  include = params$answers}

emotion_states_2st <- vit_mHMM(out_2st_emotion, emotion_mHMM)
head(emotion_states_2st)
table(emotion_states_2st$subj, emotion_states_2st$state)
```

---

To inspect the obtained most likely state sequences, plotting them is most useful. State sequences can be plotted per subject, or alternatively, together with the observed data. 

---

**Exercise 13**

Plot the inferred state sequence over a time for a selection of the subjects within the data. Tip: to plot the state sequences over time, a new variable denoting beep moment needs to be added to the matrix containing the inferred states. 

```{r,  include = params$answers}
emotion_states_2st <- data.frame(subj = emotion_states_2st$subj, state =  emotion_states_2st$state, beep = rep(c(1:240), out_2st_emotion$input$n_subj))
emotion_states_2st$state <- factor(emotion_states_2st$state, labels = paste("state", 1:m))

ggplot(emotion_states_2st[emotion_states_2st$subj %in% c(1:10),]) +
  geom_rect(aes(xmin = beep, ymin = 0, xmax = beep + 1, ymax = 1, fill = state)) +
  scale_fill_manual(values=alpha(c("state 1" = "#5ab4ac", "state 2" = "#DE7862FF"))) +
  facet_wrap(vars(subj), ncol = 1) + 
  xlab("Beep") + 
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
```

---


## Conclusion 

In this first practical, we have loaded and inspected our data, fitted a simple 2 state multilevel hidden Markov model, investigated and visualized our group level parameters, and obtained (and visualized) the most likely sequence of hidden states. In the next practical, we well focus on the model selection problem (how many states should we use), obtaining and interpreting subject specific parameters, and model checking (inspecting convergence and using posterior predictive checks to assure a good match between the model and the observed data). If you finished early with this part, feel free to already start on the second part (or take a slightly longer break). 



---

## References

- Haslbeck, J., Ryan, O., & Dablander, F. (2023). Multimodality and skewness in emotion time series. Emotion, 23(8), 2117–2141. DOI: 10.1037/emo0001218.
- Rowland, Z. & Wenzel, M. (2020). Mindfulness and affect-network density: Does mindfulness facilitate disengagement from affective experiences in daily life?. Mindfulness, 11(5), 1253-1266. DOI: 10.1007/s12671-020-01335-4
- Viterbi, A. (1967). “Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.” IEEE transactions on Information Theory, 13(2), 260–269.
